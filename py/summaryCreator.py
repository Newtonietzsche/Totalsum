# -*- coding: utf-8 -*-
"""Copy of propre_Projet_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13RiMg-MLavMLvJu0tUVgq6Cd-dC_AMv2
"""

import PyPDF2
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI
import shutil
import os
from getpass import getpass
import httpx
from gtts import gTTS
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
import os

from deep_translator import (GoogleTranslator,
                             ChatGptTranslator,
                             MicrosoftTranslator,
                             PonsTranslator,
                             LingueeTranslator,
                             MyMemoryTranslator,
                             YandexTranslator,
                             PapagoTranslator,
                             DeeplTranslator,
                             QcriTranslator,
                             single_detection,
                             batch_detection)

def get_llm_stuff(OpenAI_key):
  os.environ["OPENAI_API_KEY"] = OpenAI_key
  template = """Question: {question}

  Answer: Let's think step by step."""
  prompt = PromptTemplate.from_template(template)
  llm = OpenAI()
  llm = OpenAI(openai_api_key=OpenAI_key)
  llm_chain = LLMChain(prompt=prompt, llm=llm)
  openai = OpenAI(model_name="gpt-3.5-turbo-instruct", http_client=httpx.Client(proxies="http://proxy.yourcompany.com:8080"))
  return template, llm, llm_chain, prompt, openai

def read_pdf(file_path):
    # Open the PDF file
    with open(file_path, 'rb') as file:
        # Create a PDF reader object
        pdf_reader = PyPDF2.PdfReader(file)

        # Initialize an empty string to store the content
        pdf_content = ""

        # Iterate through each page of the PDF
        for page_num in range(len(pdf_reader.pages)):
            # Get the text content of the page
            page = pdf_reader.pages[page_num]
            page_text = page.extract_text()

            # Append the text content of the page to the variable
            pdf_content += page_text

        # Return the content of the PDF file
        return pdf_content

# Start



def register_arial_font():
    # Path to the Arial font file

    #arial_path = "Arial.ttf"  # The file should now be in the current directory
    arial_path = "./assets/fonts/Arial.ttf"


    # Register the Arial font
    pdfmetrics.registerFont(TTFont("Arial", arial_path))

def justify_line(line, width, font_name, font_size, c):
    words = line.split()
    if len(words) == 1:
        return line  # Can't justify a single word

    # Calculate total width of the words and total space width
    total_word_width = sum(c.stringWidth(word, font_name, font_size) for word in words)
    total_space_width = width - total_word_width

    # Calculate the space to add between words
    num_spaces = len(words) - 1
    space_width = total_space_width / num_spaces

    justified_line = words[0]
    for word in words[1:]:
        justified_line += " " * int(space_width / c.stringWidth(" ", font_name, font_size)) + word

    return justified_line

def create_pdf(output_text, output_file):
    print("begin create_pdf")

    # Register Arial font
    register_arial_font()

    # Create a canvas
    c = canvas.Canvas(output_file, pagesize=letter)

    # Set font and font size
    c.setFont("Arial", 12)

    # Create a text object for multiline text
    textobject = c.beginText(100, 750)  # Adjust the coordinates as needed
    max_width = 400  # Maximum width of the text line

    # Split the output text into lines
    words = output_text.split()
    line = ""
    for word in words:
        if c.stringWidth(line + " " + word, "Arial", 12) <= max_width:
            line += " " + word
        else:
            if line:
                # Justify the line
                justified_line = justify_line(line.strip(), max_width, "Arial", 12, c)
                textobject.textLine(justified_line)
            line = word

    if line:
        textobject.textLine(line.strip())  # Add the last line without justification

    # Add the text object to the canvas
    c.drawText(textobject)

    # Save the PDF
    c.save()
    print("create_pdf")

def split_text(text, chunk_size):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        if end >= len(text):
            chunks.append(text[start:])
            break
        else:
            end = text.rfind('.', start, end) + 1
            if end <= start:
                end = start + chunk_size
            chunks.append(text[start:end])
            start = end
    return chunks
def summarization(question_ori,text,chunk_size, llm_chain):
  #chunks = [text[i:i+chunk_size] for i in range(0, int(len(text)), chunk_size)]
  chunks=split_text(text, chunk_size)
  # generate a summary for each chunk

  summaries = []
  for i, chunk in enumerate(chunks):
      question = question_ori +" "+chunk
      summary = llm_chain.run(question)
      summaries.append(summary)

  # Combine the summaries
  output_text = " ".join(summaries)
  return(output_text)



def remove_incomplete_last_sentence(text):
    if text.endswith('.'):
        return text
    
    last_period = text.rfind('.')
    if last_period == -1:
        return ''  # Aucun point trouvé dans le texte, retourner une chaîne vide
    
    return text[:last_period + 1]




def trans(text,chunk_size, lang):
  chunks = [text[i:i+chunk_size] for i in range(0, int(len(text)/2), chunk_size)]



  summaries = []
  for i, chunk in enumerate(chunks):
    summary = GoogleTranslator(source='auto', target=lang).translate(text=chunk)
    summaries.append(summary)


  output_text = " ".join(summaries)
  return(output_text)

def text_to_speech(content, output_file, language):
    tts = gTTS(text=content, lang=language)
    tts.save(output_file)

def generate_files(content_path, type_of_resume, language, OpenAI_key):

    template, llm, llm_chain, prompt, openai = get_llm_stuff(OpenAI_key)

    pdf_content = read_pdf(content_path)

    output_filePdf = "summary.pdf"
    output_fileMp3 = "summary.mp3"


    
    output_text = summarization(type_of_resume,pdf_content,2400, llm_chain)
    output_text = summarization(type_of_resume,output_text,2400, llm_chain)
    output_text = summarization("Can you improve this summary",output_text,int(len(output_text)/4),llm_chain)
    output_text = summarization("Can you improve this summary",output_text,len(output_text),llm_chain)
    output_text = remove_incomplete_last_sentence(output_text)
    output_text=trans(output_text,2400, language)

    create_pdf(output_text, output_filePdf)
    
    text_to_speech(output_text, output_fileMp3, language)

    destinationPdf = os.path.join("./toDownload", "summary.pdf")
    destinationMp3 = os.path.join("./toDownload", "summary.mp3")


    # Move the files

    shutil.move(output_filePdf, destinationPdf)
    shutil.move(output_fileMp3, destinationMp3)
    
    return(output_text)

# exemple of use of the function generate_files
# generate_files('/content/Lettre de Motivation Fintech.pdf', "Can you summarize this text", 'fr', 'you_key' )


